{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_util\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow.keras as K\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILEPATH = 'models/model.h5'\n",
    "EMBED_DIM = 256\n",
    "LSTM1_SIZE = 128\n",
    "LSTM2_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_util.preprocess(load_util.load_data('deu-eng/deu.txt'))[:20000]\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(data)\n",
    "SRC, TGT = data[:, 0], data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(X):\n",
    "    maxlen = max(len(s.split()) for s in X)\n",
    "    T = K.preprocessing.text.Tokenizer()\n",
    "    T.fit_on_texts(X)\n",
    "    X_enc = T.texts_to_sequences(X)\n",
    "    X_enc_pad = K.preprocessing.sequence.pad_sequences(X_enc, maxlen=maxlen, padding='post')\n",
    "    return X_enc_pad, T\n",
    "\n",
    "def onehot_3d(X, vocab_len):\n",
    "    onehot = np.array([K.utils.to_categorical(seq, vocab_len) for seq in X])\n",
    "    return onehot\n",
    "\n",
    "def vocab_len(X):\n",
    "    return len({num for seq in X for num in seq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_enc_pad, SRC_tokenizer = encode_and_pad(SRC)\n",
    "TGT_enc_pad, TGT_tokenizer = encode_and_pad(TGT)\n",
    "\n",
    "SRC_vocab_len = vocab_len(SRC_enc_pad)\n",
    "TGT_vocab_len = vocab_len(TGT_enc_pad)\n",
    "\n",
    "SRC_enc_pad_onehot = onehot_3d(SRC_enc_pad, SRC_vocab_len)\n",
    "TGT_enc_pad_onehot = onehot_3d(TGT_enc_pad, TGT_vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(X, Y, train_size=.9):\n",
    "    assert len(SRC_enc_pad) == len(TGT_enc_pad)\n",
    "    cutoff = int(len(X) * train_size)\n",
    "    return X[:cutoff], X[cutoff:], Y[:cutoff], Y[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_train, SRC_test, TGT_train, TGT_test = train_split(SRC_enc_pad, TGT_enc_pad, train_size=.9)\n",
    "\n",
    "TGT_train_onehot = onehot_3d(TGT_train, TGT_vocab_len)\n",
    "TGT_test_onehot = onehot_3d(TGT_train, TGT_vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_vocab_len,\n",
    "    input_max_len,\n",
    "    output_vocab_len,\n",
    "    output_max_len,\n",
    "    embed_dim,\n",
    "    lstm1_units,\n",
    "    lstm2_units):\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Embedding(\n",
    "        input_dim=input_vocab_len,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        input_length=input_max_len))\n",
    "    model.add(K.layers.LSTM(units=lstm1_units))\n",
    "    model.add(K.layers.RepeatVector(n=output_max_len))\n",
    "    model.add(K.layers.LSTM(units=lstm2_units, return_sequences=True))\n",
    "    model.add(K.layers.TimeDistributed(K.layers.Dense(output_vocab_len, activation='softmax')))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5, 256)            944384    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           131584    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 5742)          740718    \n",
      "=================================================================\n",
      "Total params: 2,013,806\n",
      "Trainable params: 2,013,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_vocab_len=SRC_vocab_len,\n",
    "    input_max_len=SRC_train.shape[1],\n",
    "    output_vocab_len=TGT_vocab_len,\n",
    "    output_max_len=TGT_train_onehot.shape[1],\n",
    "    embed_dim=EMBED_DIM,\n",
    "    lstm1_units=LSTM1_SIZE,\n",
    "    lstm2_units=LSTM2_SIZE\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 3.2036\n",
      "Epoch 00001: val_loss improved from inf to 2.48037, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 154s 9ms/sample - loss: 3.2031 - val_loss: 2.4804\n",
      "Epoch 2/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 2.3573\n",
      "Epoch 00002: val_loss improved from 2.48037 to 2.30046, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 140s 8ms/sample - loss: 2.3572 - val_loss: 2.3005\n",
      "Epoch 3/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 2.2232\n",
      "Epoch 00003: val_loss improved from 2.30046 to 2.21923, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 137s 8ms/sample - loss: 2.2231 - val_loss: 2.2192\n",
      "Epoch 4/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 2.1366\n",
      "Epoch 00004: val_loss improved from 2.21923 to 2.11934, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 136s 8ms/sample - loss: 2.1366 - val_loss: 2.1193\n",
      "Epoch 5/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 2.0118\n",
      "Epoch 00005: val_loss improved from 2.11934 to 2.02586, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 137s 8ms/sample - loss: 2.0119 - val_loss: 2.0259\n",
      "Epoch 6/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.9233\n",
      "Epoch 00006: val_loss improved from 2.02586 to 1.95704, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 137s 8ms/sample - loss: 1.9231 - val_loss: 1.9570\n",
      "Epoch 7/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.8234\n",
      "Epoch 00007: val_loss improved from 1.95704 to 1.86880, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 142s 8ms/sample - loss: 1.8235 - val_loss: 1.8688\n",
      "Epoch 8/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.7317\n",
      "Epoch 00008: val_loss improved from 1.86880 to 1.80243, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 141s 8ms/sample - loss: 1.7317 - val_loss: 1.8024\n",
      "Epoch 9/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.6492\n",
      "Epoch 00009: val_loss improved from 1.80243 to 1.73454, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 143s 8ms/sample - loss: 1.6491 - val_loss: 1.7345\n",
      "Epoch 10/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.5651\n",
      "Epoch 00010: val_loss improved from 1.73454 to 1.67097, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 144s 8ms/sample - loss: 1.5654 - val_loss: 1.6710\n",
      "Epoch 11/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.4842\n",
      "Epoch 00011: val_loss improved from 1.67097 to 1.60666, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 143s 8ms/sample - loss: 1.4842 - val_loss: 1.6067\n",
      "Epoch 12/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.4051\n",
      "Epoch 00012: val_loss improved from 1.60666 to 1.54894, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 140s 8ms/sample - loss: 1.4051 - val_loss: 1.5489\n",
      "Epoch 13/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.3266\n",
      "Epoch 00013: val_loss improved from 1.54894 to 1.50039, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 142s 8ms/sample - loss: 1.3264 - val_loss: 1.5004\n",
      "Epoch 14/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.2520\n",
      "Epoch 00014: val_loss improved from 1.50039 to 1.45776, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 141s 8ms/sample - loss: 1.2518 - val_loss: 1.4578\n",
      "Epoch 15/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.1814\n",
      "Epoch 00015: val_loss improved from 1.45776 to 1.41639, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 143s 8ms/sample - loss: 1.1815 - val_loss: 1.4164\n",
      "Epoch 16/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.1169\n",
      "Epoch 00016: val_loss improved from 1.41639 to 1.38769, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 146s 8ms/sample - loss: 1.1168 - val_loss: 1.3877\n",
      "Epoch 17/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 1.0560\n",
      "Epoch 00017: val_loss improved from 1.38769 to 1.35125, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 147s 8ms/sample - loss: 1.0561 - val_loss: 1.3512\n",
      "Epoch 18/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.9974\n",
      "Epoch 00018: val_loss improved from 1.35125 to 1.32748, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 147s 8ms/sample - loss: 0.9973 - val_loss: 1.3275\n",
      "Epoch 19/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.9413\n",
      "Epoch 00019: val_loss improved from 1.32748 to 1.30135, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 161s 9ms/sample - loss: 0.9414 - val_loss: 1.3013\n",
      "Epoch 20/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.8874\n",
      "Epoch 00020: val_loss improved from 1.30135 to 1.28037, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 149s 8ms/sample - loss: 0.8873 - val_loss: 1.2804\n",
      "Epoch 21/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.8351\n",
      "Epoch 00021: val_loss improved from 1.28037 to 1.25926, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 151s 8ms/sample - loss: 0.8350 - val_loss: 1.2593\n",
      "Epoch 22/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.7881\n",
      "Epoch 00022: val_loss improved from 1.25926 to 1.24251, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 152s 8ms/sample - loss: 0.7880 - val_loss: 1.2425\n",
      "Epoch 23/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.7411\n",
      "Epoch 00023: val_loss improved from 1.24251 to 1.22793, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 150s 8ms/sample - loss: 0.7412 - val_loss: 1.2279\n",
      "Epoch 24/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.6992\n",
      "Epoch 00024: val_loss improved from 1.22793 to 1.21308, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 145s 8ms/sample - loss: 0.6993 - val_loss: 1.2131\n",
      "Epoch 25/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.6588\n",
      "Epoch 00025: val_loss improved from 1.21308 to 1.20216, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 149s 8ms/sample - loss: 0.6588 - val_loss: 1.2022\n",
      "Epoch 26/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.6223\n",
      "Epoch 00026: val_loss improved from 1.20216 to 1.19131, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 149s 8ms/sample - loss: 0.6224 - val_loss: 1.1913\n",
      "Epoch 27/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.5878\n",
      "Epoch 00027: val_loss improved from 1.19131 to 1.18469, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 145s 8ms/sample - loss: 0.5880 - val_loss: 1.1847\n",
      "Epoch 28/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.5559\n",
      "Epoch 00028: val_loss improved from 1.18469 to 1.17781, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 143s 8ms/sample - loss: 0.5558 - val_loss: 1.1778\n",
      "Epoch 29/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.5266\n",
      "Epoch 00029: val_loss improved from 1.17781 to 1.17417, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 141s 8ms/sample - loss: 0.5267 - val_loss: 1.1742\n",
      "Epoch 30/30\n",
      "17984/18000 [============================>.] - ETA: 0s - loss: 0.4999\n",
      "Epoch 00030: val_loss improved from 1.17417 to 1.16916, saving model to models/model.h5\n",
      "18000/18000 [==============================] - 150s 8ms/sample - loss: 0.4999 - val_loss: 1.1692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6469e6810>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = K.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_FILEPATH,\n",
    "    monitor='val_loss',\n",
    "    verbose=True,\n",
    "    save_best_only=True)\n",
    "model.fit(\n",
    "    x=SRC_train,\n",
    "    y=TGT_train_onehot,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    verbose=True,\n",
    "    callbacks=[model_checkpoint],\n",
    "    validation_data=[SRC_test, TGT_test_onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(true, pred):\n",
    "    weights = [\n",
    "        (1,     0,   0,   0),\n",
    "        (1/2, 1/2,   0,   0),\n",
    "        (1/3, 1/3, 1/3,   0),\n",
    "        (1/4, 1/4, 1/4, 1/4)\n",
    "    ]\n",
    "    return [corpus_bleu(true, pred, w) for w in weights]\n",
    "\n",
    "def evaluate(model, X, Y, X_tokenizer, Y_tokenizer):\n",
    "    SRC_idx2word = {v:k for k, v in X_tokenizer.word_index.items()}\n",
    "    TGT_idx2word = {v:k for k, v in Y_tokenizer.word_index.items()}\n",
    "    predictions = model.predict(X)\n",
    "    y = []\n",
    "    y_pred = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        src_sent = [SRC_idx2word[idx] for idx in X[i] if idx in SRC_idx2word]\n",
    "        tgt_sent = [TGT_idx2word[idx] for idx in Y[i] if idx in TGT_idx2word]\n",
    "        tgt_pred = [np.argmax(val) for val in pred]\n",
    "        tgt_pred = [TGT_idx2word[idx] for idx in tgt_pred if idx in TGT_idx2word]\n",
    "        y.append([tgt_sent])\n",
    "        y_pred.append(tgt_pred)\n",
    "        if i < 20:\n",
    "            print(' '.join(tgt_sent), ' --> ', ' '.join(tgt_pred))\n",
    "    bleu_scores = bleu(y, y_pred)\n",
    "    for idx, bleu_score in enumerate(bleu_scores):\n",
    "        print('{}-gram BLEU: {:.4f}'.format(idx + 1, bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er starb gestern er ist gestern gestorben\n",
      "ich habe zweifel ich habe zweifel\n",
      "tom erblindete tom erblindete\n",
      "er kann dir nicht helfen er kann dir nicht helfen\n",
      "ich bin kein fachmann ich bin kein fachmann fach\n",
      "seid tapfer seid tapfer\n",
      "hndige es aus hndige es aus\n",
      "lasst tom nachhause gehen lass tom nachhause gehen\n",
      "dein hund hat mich gebissen die hund hat mich gebissen\n",
      "tom ist gemein tom ist gemein\n",
      "1-gram BLEU: 0.7341\n",
      "2-gram BLEU: 0.6401\n",
      "3-gram BLEU: 0.5506\n",
      "4-gram BLEU: 0.4073\n"
     ]
    }
   ],
   "source": [
    "evaluate(\n",
    "    model=finalized_model,\n",
    "    X=SRC_train,\n",
    "    Y=TGT_train,\n",
    "    X_tokenizer=SRC_tokenizer,\n",
    "    Y_tokenizer=TGT_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom hat einen dachschaden tom hat ein wahnsinniger\n",
      "bin ich frh dran bin ich frh\n",
      "ich bin kein spion ich bin ein feigling\n",
      "ich mag knoblauch ich liebe australien\n",
      "bist du in gefahr sind sie in gefahr\n",
      "fhre mich nicht in versuchung fhren sie mich mich mich versuchung\n",
      "stimmt das nicht ist es nicht wahr\n",
      "tom war naiv tom war naiv\n",
      "er verlie das zimmer er ist den dem\n",
      "ich sehe den jungen ich sehe auto\n",
      "1-gram BLEU: 0.5429\n",
      "2-gram BLEU: 0.4113\n",
      "3-gram BLEU: 0.3002\n",
      "4-gram BLEU: 0.1873\n"
     ]
    }
   ],
   "source": [
    "evaluate(\n",
    "    model=finalized_model,\n",
    "    X=SRC_test,\n",
    "    Y=TGT_test,\n",
    "    X_tokenizer=SRC_tokenizer,\n",
    "    Y_tokenizer=TGT_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
