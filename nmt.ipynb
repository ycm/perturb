{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbed embeddings\n",
    "## Novel approaches in data augmentation for low-resource machine translation\n",
    "\n",
    "Andrew Yang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_util import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras as K\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILEPATH = 'models/full-data-model.h5'\n",
    "EMBED_DIM = 300\n",
    "LSTM1_SIZE = 256\n",
    "LSTM2_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204574, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = preprocess(load_data('deu-eng/deu.txt'))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:30000]\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(data)\n",
    "SRC, TGT = data[:, 0], data[:, 1]\n",
    "SRC_enc_pad, SRC_tokenizer = encode_and_pad(SRC)\n",
    "TGT_enc_pad, TGT_tokenizer = encode_and_pad(TGT)\n",
    "SRC_vocab_len = vocab_len(SRC_enc_pad)\n",
    "TGT_vocab_len = vocab_len(TGT_enc_pad)\n",
    "SRC_train, SRC_test, TGT_train, TGT_test = train_split(SRC_enc_pad, TGT_enc_pad, train_size=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TGT_train_onehot = onehot_3d(TGT_train, TGT_vocab_len)\n",
    "TGT_test_onehot = onehot_3d(TGT_test, TGT_vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_vocab_len,\n",
    "    input_max_len,\n",
    "    output_vocab_len,\n",
    "    output_max_len,\n",
    "    embed_dim,\n",
    "    lstm1_units,\n",
    "    lstm2_units):\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Embedding(\n",
    "        input_dim=input_vocab_len,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        input_length=input_max_len))\n",
    "    model.add(K.layers.LSTM(units=lstm1_units))\n",
    "    model.add(K.layers.RepeatVector(n=output_max_len))\n",
    "    model.add(K.layers.LSTM(units=lstm2_units, return_sequences=True))\n",
    "    model.add(K.layers.TimeDistributed(K.layers.Dense(output_vocab_len, activation='softmax')))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 6, 300)            1407900   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 7432)          1910024   \n",
      "=================================================================\n",
      "Total params: 4,413,604\n",
      "Trainable params: 4,413,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_vocab_len=SRC_vocab_len,\n",
    "    input_max_len=SRC_train.shape[1],\n",
    "    output_vocab_len=TGT_vocab_len,\n",
    "    output_max_len=TGT_train_onehot.shape[1],\n",
    "    embed_dim=EMBED_DIM,\n",
    "    lstm1_units=LSTM1_SIZE,\n",
    "    lstm2_units=LSTM2_SIZE\n",
    ")\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27000 samples, validate on 3000 samples\n",
      "Epoch 1/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 2.8724\n",
      "Epoch 00001: val_loss improved from inf to 2.43471, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 32s 1ms/sample - loss: 2.8715 - val_loss: 2.4347\n",
      "Epoch 2/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 2.3369\n",
      "Epoch 00002: val_loss improved from 2.43471 to 2.27912, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 494us/sample - loss: 2.3369 - val_loss: 2.2791\n",
      "Epoch 3/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 2.1541\n",
      "Epoch 00003: val_loss improved from 2.27912 to 2.13746, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 491us/sample - loss: 2.1537 - val_loss: 2.1375\n",
      "Epoch 4/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 1.9916\n",
      "Epoch 00004: val_loss improved from 2.13746 to 1.96214, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 490us/sample - loss: 1.9918 - val_loss: 1.9621\n",
      "Epoch 5/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 1.8028\n",
      "Epoch 00005: val_loss improved from 1.96214 to 1.80343, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 487us/sample - loss: 1.8029 - val_loss: 1.8034\n",
      "Epoch 6/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 1.6314\n",
      "Epoch 00006: val_loss improved from 1.80343 to 1.67410, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 490us/sample - loss: 1.6315 - val_loss: 1.6741\n",
      "Epoch 7/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 1.4773\n",
      "Epoch 00007: val_loss improved from 1.67410 to 1.55695, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 490us/sample - loss: 1.4770 - val_loss: 1.5569\n",
      "Epoch 8/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 1.3369\n",
      "Epoch 00008: val_loss improved from 1.55695 to 1.46376, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 486us/sample - loss: 1.3368 - val_loss: 1.4638\n",
      "Epoch 9/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 1.2127\n",
      "Epoch 00009: val_loss improved from 1.46376 to 1.38792, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 490us/sample - loss: 1.2127 - val_loss: 1.3879\n",
      "Epoch 10/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 1.0990\n",
      "Epoch 00010: val_loss improved from 1.38792 to 1.32808, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 490us/sample - loss: 1.0988 - val_loss: 1.3281\n",
      "Epoch 11/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.9944\n",
      "Epoch 00011: val_loss improved from 1.32808 to 1.26977, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 487us/sample - loss: 0.9943 - val_loss: 1.2698\n",
      "Epoch 12/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.8994\n",
      "Epoch 00012: val_loss improved from 1.26977 to 1.22671, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 489us/sample - loss: 0.8998 - val_loss: 1.2267\n",
      "Epoch 13/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.8141\n",
      "Epoch 00013: val_loss improved from 1.22671 to 1.18742, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 488us/sample - loss: 0.8142 - val_loss: 1.1874\n",
      "Epoch 14/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.7379\n",
      "Epoch 00014: val_loss improved from 1.18742 to 1.15824, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 488us/sample - loss: 0.7381 - val_loss: 1.1582\n",
      "Epoch 15/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.6692\n",
      "Epoch 00015: val_loss improved from 1.15824 to 1.13456, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 487us/sample - loss: 0.6695 - val_loss: 1.1346\n",
      "Epoch 16/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.6099\n",
      "Epoch 00016: val_loss improved from 1.13456 to 1.11786, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 494us/sample - loss: 0.6099 - val_loss: 1.1179\n",
      "Epoch 17/40\n",
      "26880/27000 [============================>.] - ETA: 0s - loss: 0.5578\n",
      "Epoch 00017: val_loss improved from 1.11786 to 1.10154, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 486us/sample - loss: 0.5577 - val_loss: 1.1015\n",
      "Epoch 18/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.5126\n",
      "Epoch 00018: val_loss improved from 1.10154 to 1.09773, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 489us/sample - loss: 0.5125 - val_loss: 1.0977\n",
      "Epoch 19/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.4744\n",
      "Epoch 00019: val_loss improved from 1.09773 to 1.09105, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 485us/sample - loss: 0.4745 - val_loss: 1.0911\n",
      "Epoch 20/40\n",
      "26880/27000 [============================>.] - ETA: 0s - loss: 0.4394\n",
      "Epoch 00020: val_loss improved from 1.09105 to 1.08463, saving model to models/full-data-model.h5\n",
      "27000/27000 [==============================] - 13s 493us/sample - loss: 0.4396 - val_loss: 1.0846\n",
      "Epoch 21/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.4099\n",
      "Epoch 00021: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 481us/sample - loss: 0.4100 - val_loss: 1.0874\n",
      "Epoch 22/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.3849\n",
      "Epoch 00022: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 482us/sample - loss: 0.3849 - val_loss: 1.0862\n",
      "Epoch 23/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.3613\n",
      "Epoch 00023: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 485us/sample - loss: 0.3615 - val_loss: 1.0848\n",
      "Epoch 24/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.3415\n",
      "Epoch 00024: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 480us/sample - loss: 0.3415 - val_loss: 1.0943\n",
      "Epoch 25/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.3246\n",
      "Epoch 00025: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 477us/sample - loss: 0.3248 - val_loss: 1.0936\n",
      "Epoch 26/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.3080\n",
      "Epoch 00026: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 482us/sample - loss: 0.3082 - val_loss: 1.1033\n",
      "Epoch 27/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.2946\n",
      "Epoch 00027: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 482us/sample - loss: 0.2946 - val_loss: 1.1048\n",
      "Epoch 28/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.2809\n",
      "Epoch 00028: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 478us/sample - loss: 0.2810 - val_loss: 1.1098\n",
      "Epoch 29/40\n",
      "26944/27000 [============================>.] - ETA: 0s - loss: 0.2695\n",
      "Epoch 00029: val_loss did not improve from 1.08463\n",
      "27000/27000 [==============================] - 13s 483us/sample - loss: 0.2696 - val_loss: 1.1215\n",
      "Epoch 30/40\n",
      " 1344/27000 [>.............................] - ETA: 11s - loss: 0.2360WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1408/27000 [>.............................] - ETA: 12s - loss: 0.2342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-25604791e821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     validation_data=[SRC_test, TGT_test_onehot])\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[1;32m    173\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_exhausted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         self.callbacks._call_batch_hook(\n\u001b[0;32m--> 700\u001b[0;31m             mode, 'end', step, batch_logs)\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    240\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3500\u001b[0m     \"\"\"\n\u001b[1;32m   3501\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3502\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3503\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3384\u001b[0m     \"\"\"\n\u001b[0;32m-> 3385\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3386\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_checkpoint = K.callbacks.ModelCheckpoint(filepath=MODEL_FILEPATH, monitor='val_loss', verbose=True, save_best_only=True)\n",
    "model.fit(\n",
    "    x=SRC_train,\n",
    "    y=TGT_train_onehot,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    verbose=True,\n",
    "    callbacks=[model_checkpoint],\n",
    "    validation_data=[SRC_test, TGT_test_onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = K.models.load_model(MODEL_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halten sie mir einen platz frei  -->  halt mir einen platz frei\n",
      "ich muss ihn warnen  -->  ich muss ihn warnen\n",
      "tom war stark  -->  tom war stark\n",
      "lassen sie mich das erledigen  -->  lass mich das das\n",
      "sie belieben wohl zu scherzen  -->  du beliebt wohl zu scherzen\n",
      "verpetzen sie mich nicht  -->  verpetz mich nicht\n",
      "verpiss dich  -->  mach dich\n",
      "tom ist wieder da  -->  tom ist wieder\n",
      "ich bin arzt  -->  ich bin arzt\n",
      "du bist krank  -->  du bist krank\n",
      "tom spielt rugby  -->  tom spielt kricket\n",
      "wo sind die schlssel  -->  wo sind die schlssel\n",
      "es ist jetzt da  -->  es ist jetzt da\n",
      "tom war ein starker trinker  -->  tom war ein starker\n",
      "sie faszinierte mich  -->  sie faszinierte mich\n",
      "tom ist leichtglubig  -->  tom ist leichtglubig\n",
      "er ist ein schneller lufer  -->  er rennt schnell\n",
      "ihr habt pech  -->  du hast pech\n",
      "ich werde dich anrufen  -->  ich werde dich anrufen\n",
      "alle wissen es  -->  alle wissen es\n",
      "1-gram BLEU: 0.7737\n",
      "2-gram BLEU: 0.6900\n",
      "3-gram BLEU: 0.6120\n",
      "4-gram BLEU: 0.4914\n"
     ]
    }
   ],
   "source": [
    "evaluate(model=trained_model, X=SRC_train, Y=TGT_train, X_tokenizer=SRC_tokenizer, Y_tokenizer=TGT_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgel singen  -->  vgel\n",
      "ich kannte tom nicht  -->  ich habe tom nicht gekannt\n",
      "wer ist eure freundin  -->  wer ist euer freundin\n",
      "tom wrde es wissen  -->  tom wrde es\n",
      "tom warf den ball  -->  tom hat den ball\n",
      "beruhigen sie sich  -->  beruhigen sie\n",
      "ich helfe  -->  ich werde helfen\n",
      "kann ich ihnen helfen  -->  darf ich dir helfen\n",
      "tom bat um hilfe  -->  tom hat mit nicht\n",
      "lass uns diesen kaufen  -->  lasst uns uns kaufen\n",
      "tom hat veranlasst dass maria geht  -->  tom hat maria tom zu gebracht\n",
      "dies ist nicht mein haus  -->  das ist nicht nicht tom\n",
      "tom hat gebeichtet  -->  tom hat kaninchen\n",
      "maria singt alt  -->  sie singt sopran\n",
      "tom wird heulen  -->  tom wird weinen\n",
      "er hat kein fahrrad  -->  er hat keinen ein\n",
      "du liest meine gedanken  -->  du brauchen mir ein\n",
      "ich bin eine optimistin  -->  ich bin ein optimist\n",
      "tom ist gefunden worden  -->  tom ist gefunden\n",
      "ich mag den hund  -->  ich mag die hund\n",
      "1-gram BLEU: 0.5832\n",
      "2-gram BLEU: 0.4602\n",
      "3-gram BLEU: 0.3554\n",
      "4-gram BLEU: 0.2478\n"
     ]
    }
   ],
   "source": [
    "evaluate(model=trained_model, X=SRC_test, Y=TGT_test, X_tokenizer=SRC_tokenizer, Y_tokenizer=TGT_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
